Self-critique mechanisms have emerged as a promising approach for improving large language model (LLM) outputs without additional training. We investigate whether prompting LLMs to be harsher critics of their own work improves output quality compared to neutral self-critique. Using a generate-critique-refine framework, we test five levels of critique harshness---from neutral to adversarial---on two tasks: \gsm (math reasoning) and \truthful (factual accuracy). Our results reveal a striking task-dependent effect: harsh self-critique significantly \emph{degrades} performance on math reasoning (from 90\% to 32--50\%) while significantly \emph{improving} performance on factual accuracy tasks (from 22\% to 46\%). We find that the key determinant is initial accuracy: harsh critique helps when the model is likely to be wrong but harms when the initial answer is likely correct. External rudeness from users has no effect on either task, suggesting that the mechanism of improvement is internal reconsideration rather than effort increase. These findings challenge the simple intuition that being harder on oneself leads to better work, and provide practical guidance for when harsh self-critique should and should not be employed.
